{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QfutrjrU321",
        "outputId": "3b82b93e-6ba0-45fc-b8cb-8441ee402c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ASSIGNMENT 2: GRAPH ANALYSIS WITHOUT GRAPH LIBRARIES\n",
            "============================================================\n",
            "Spark Version: 3.5.1\n",
            "Only using core Spark functionality - no graph libraries!\n",
            "============================================================\n",
            "--2025-10-05 14:16:11--  https://snap.stanford.edu/data/wiki-Vote.txt.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 290339 (284K) [application/x-gzip]\n",
            "Saving to: ‘wiki-Vote.txt.gz’\n",
            "\n",
            "wiki-Vote.txt.gz    100%[===================>] 283.53K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-10-05 14:16:11 (1.89 MB/s) - ‘wiki-Vote.txt.gz’ saved [290339/290339]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# PART 1: SETUP AND IMPORTS\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict, deque\n",
        "from itertools import combinations\n",
        "\n",
        "# Spark imports ONLY - no graph libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"WikiVoteNetworkNoGraphLibs\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ASSIGNMENT 2: GRAPH ANALYSIS WITHOUT GRAPH LIBRARIES\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(\"Only using core Spark functionality - no graph libraries!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "!wget -nc https://snap.stanford.edu/data/wiki-Vote.txt.gz\n",
        "!gunzip -k wiki-Vote.txt.gz\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t4uenrQX70W",
        "outputId": "db21735f-c0af-483f-e647-787b0787c49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "LOADING DATA\n",
            "==================================================\n",
            "Edges loaded: 103689\n",
            "+---+----+\n",
            "|src| dst|\n",
            "+---+----+\n",
            "|  6| 274|\n",
            "|  6| 826|\n",
            "|  7|  55|\n",
            "| 11|1248|\n",
            "| 17|1218|\n",
            "+---+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# PART 2: DATA LOADING (Same as Assignment 1)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def load_wiki_vote_data(filepath):\n",
        "    \"\"\"Load wiki-Vote.txt file using only Spark RDDs/DataFrames\"\"\"\n",
        "\n",
        "    # Read raw text file\n",
        "    raw_rdd = spark.sparkContext.textFile(filepath)\n",
        "\n",
        "    # Filter out comments and empty lines\n",
        "    edges_rdd = raw_rdd.filter(lambda line: not line.startswith('#') and len(line.strip()) > 0)\n",
        "\n",
        "    # Parse edges\n",
        "    edges_parsed = edges_rdd.map(lambda line: line.strip().split('\\t')) \\\n",
        "                            .filter(lambda x: len(x) == 2) \\\n",
        "                            .map(lambda x: (int(x[0]), int(x[1])))\n",
        "\n",
        "    # Create DataFrame\n",
        "    schema = StructType([\n",
        "        StructField(\"src\", IntegerType(), False),\n",
        "        StructField(\"dst\", IntegerType(), False)\n",
        "    ])\n",
        "\n",
        "    edges_df = spark.createDataFrame(edges_parsed, schema)\n",
        "    edges_df = edges_df.distinct()\n",
        "\n",
        "    return edges_df\n",
        "\n",
        "# Load the data\n",
        "\n",
        "edges_df = load_wiki_vote_data(\"/content/wiki-Vote.txt\")\n",
        "edges_df.cache()\n",
        "edges_df.createOrReplaceTempView(\"edges\")\n",
        "\n",
        "print(f\"Edges loaded: {edges_df.count()}\")\n",
        "edges_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VoQZ5ShYBZ4",
        "outputId": "58f76a9b-6316-46fe-b538-825a8f417b21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "COMPUTING BASIC STATISTICS\n",
            "==================================================\n",
            "Number of edges: 103689\n",
            "Number of nodes: 7115\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# PART 3: BASIC COUNTS - NODES AND EDGES\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPUTING BASIC STATISTICS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Count edges\n",
        "num_edges = edges_df.count()\n",
        "print(f\"Number of edges: {num_edges}\")\n",
        "\n",
        "# Count unique nodes using SQL\n",
        "node_count_query = \"\"\"\n",
        "SELECT COUNT(DISTINCT node) as node_count\n",
        "FROM (\n",
        "    SELECT src as node FROM edges\n",
        "    UNION\n",
        "    SELECT dst as node FROM edges\n",
        ") nodes\n",
        "\"\"\"\n",
        "num_nodes = spark.sql(node_count_query).collect()[0]['node_count']\n",
        "print(f\"Number of nodes: {num_nodes}\")\n",
        "\n",
        "# Create vertices DataFrame\n",
        "vertices_df = spark.sql(\"\"\"\n",
        "    SELECT DISTINCT node as id\n",
        "    FROM (\n",
        "        SELECT src as node FROM edges\n",
        "        UNION\n",
        "        SELECT dst as node FROM edges\n",
        "    ) nodes\n",
        "\"\"\")\n",
        "vertices_df.cache()\n",
        "vertices_df.createOrReplaceTempView(\"vertices\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uX4gYzEYsyp",
        "outputId": "973b712d-f901-438c-bea9-9cf404a72967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "COMPUTING WEAKLY CONNECTED COMPONENTS (Optimized Parallel)\n",
            "==================================================\n",
            "Iteration 1: 7115 active nodes\n",
            "Iteration 2: 6166 active nodes\n",
            "Iteration 3: 7016 active nodes\n",
            "Iteration 4: 5816 active nodes\n",
            "Iteration 5: 1338 active nodes\n",
            "Iteration 6: 15 active nodes\n",
            "\n",
            "Largest WCC has 7066 nodes\n",
            "Fraction of nodes in largest WCC: 0.993\n",
            "Edges in largest WCC: 103663\n",
            "Fraction of edges in largest WCC: 1.000\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# PART 4: WEAKLY CONNECTED COMPONENTS (Optimized Parallel)\n",
        "# Using Union-Find style iterative propagation\n",
        "# ============================================\n",
        "import math\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPUTING WEAKLY CONNECTED COMPONENTS (Optimized Parallel)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def compute_wcc_parallel():\n",
        "    \"\"\"\n",
        "    Compute Weakly Connected Components using iterative propagation.\n",
        "    - Undirected edge expansion\n",
        "    - Union-Find style label minimization\n",
        "    - Delta iteration (only changed nodes)\n",
        "    - Logarithmic iteration cap\n",
        "    \"\"\"\n",
        "\n",
        "    undirected_edges = (\n",
        "        edges_df.select(\"src\", \"dst\")\n",
        "        .union(edges_df.select(F.col(\"dst\").alias(\"src\"), F.col(\"src\").alias(\"dst\")))\n",
        "        .distinct()\n",
        "        .cache()\n",
        "    )\n",
        "\n",
        "    components = vertices_df.select(\n",
        "        F.col(\"id\").alias(\"node\"), F.col(\"id\").alias(\"component\")\n",
        "    )\n",
        "\n",
        "    active = components.select(\"node\")\n",
        "    max_iter = int(math.ceil(math.log2(num_nodes))) + 2\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        active_count = active.count()\n",
        "        if active_count == 0:\n",
        "            break\n",
        "        print(f\"Iteration {i+1}: {active_count} active nodes\")\n",
        "\n",
        "        neighbor_updates = (\n",
        "            undirected_edges.join(active, undirected_edges.src == active.node, \"inner\")\n",
        "            .join(components.withColumnRenamed(\"node\", \"src\"), \"src\")\n",
        "            .select(\"dst\", \"component\")\n",
        "            .groupBy(\"dst\")\n",
        "            .agg(F.min(\"component\").alias(\"new_component\"))\n",
        "        )\n",
        "\n",
        "        updated = (\n",
        "            components.join(neighbor_updates, components.node == neighbor_updates.dst, \"left\")\n",
        "            .select(\n",
        "                components.node,\n",
        "                F.least(components.component, F.coalesce(F.col(\"new_component\"), components.component)).alias(\"component\")\n",
        "            )\n",
        "        )\n",
        "\n",
        "        active = (\n",
        "            components.join(updated, \"node\")\n",
        "            .where(components.component != updated.component)\n",
        "            .select(\"node\")\n",
        "            .distinct()\n",
        "        )\n",
        "\n",
        "        components = updated.cache()\n",
        "\n",
        "    wcc_sizes = (\n",
        "        components.groupBy(\"component\")\n",
        "        .agg(F.count(\"*\").alias(\"size\"))\n",
        "        .orderBy(F.desc(\"size\"))\n",
        "    )\n",
        "\n",
        "    largest = wcc_sizes.first()\n",
        "    largest_id, largest_size = largest[\"component\"], largest[\"size\"]\n",
        "\n",
        "    edges_in_wcc = (\n",
        "        edges_df\n",
        "        .join(\n",
        "            components.withColumnRenamed(\"node\", \"src\")\n",
        "                      .withColumnRenamed(\"component\", \"src_component\"),\n",
        "            \"src\"\n",
        "        )\n",
        "        .join(\n",
        "            components.withColumnRenamed(\"node\", \"dst\")\n",
        "                      .withColumnRenamed(\"component\", \"dst_component\"),\n",
        "            \"dst\"\n",
        "        )\n",
        "        .where(F.col(\"src_component\") == F.col(\"dst_component\"))\n",
        "        .filter(F.col(\"src_component\") == largest_id)\n",
        "        .count()\n",
        "    )\n",
        "\n",
        "    return largest_size, edges_in_wcc, components\n",
        "\n",
        "\n",
        "largest_wcc_size, edges_in_wcc, wcc_components = compute_wcc_parallel()\n",
        "print(f\"\\nLargest WCC has {largest_wcc_size} nodes\")\n",
        "print(f\"Fraction of nodes in largest WCC: {largest_wcc_size/num_nodes:.3f}\")\n",
        "print(f\"Edges in largest WCC: {edges_in_wcc}\")\n",
        "print(f\"Fraction of edges in largest WCC: {edges_in_wcc/num_edges:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8Fq2_-dAYzxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a75775e-4de5-454d-f572-74d510758754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "COMPUTING STRONGLY CONNECTED COMPONENTS\n",
            "==================================================\n",
            "  Building graph structures...\n",
            "  Computing finish order approximation...\n",
            "  Initializing components...\n",
            "  Running parallel label propagation on reverse graph...\n",
            "    Iteration 1: 4943 components\n",
            "    Iteration 2: 4756 components\n",
            "    Iteration 3: 4744 components\n",
            "    Iteration 4: 4742 components\n",
            "    Iteration 5: 4741 components\n",
            "    Iteration 6: 4741 components\n",
            "    Converged after 6 iterations\n",
            "  Computing SCC statistics...\n",
            "\n",
            "Largest SCC has 2316 nodes\n",
            "Fraction of nodes in largest SCC: 0.326\n",
            "Edges in largest SCC: 57650\n",
            "Fraction of edges in largest SCC: 0.556\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# PART 5: STRONGLY CONNECTED COMPONENTS\n",
        "# Optimized Kosaraju's Algorithm with Parallelization\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPUTING STRONGLY CONNECTED COMPONENTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "from pyspark.sql.functions import col, lit, collect_list, min as spark_min, explode, array_distinct, flatten, when, size, array, least\n",
        "\n",
        "def compute_scc_kosaraju_optimized():\n",
        "    \"\"\"\n",
        "    Optimized Kosaraju's algorithm:\n",
        "    1. Build adjacency lists efficiently (cached, broadcast if small)\n",
        "    2. Use GraphFrames for SCC if available, else optimized iterative approach\n",
        "    3. Parallel component propagation with better convergence\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"  Building graph structures...\")\n",
        "\n",
        "    # Build forward and reverse adjacency lists in one pass\n",
        "    forward_edges = edges_df.groupBy(\"src\").agg(collect_list(\"dst\").alias(\"neighbors\"))\n",
        "    reverse_edges = edges_df.groupBy(\"dst\").agg(collect_list(\"src\").alias(\"neighbors\"))\n",
        "\n",
        "    # Join with all vertices to include isolated nodes\n",
        "    from pyspark.sql.functions import when, size, array\n",
        "\n",
        "    forward_adj = vertices_df.select(col(\"id\").alias(\"node\")) \\\n",
        "        .join(forward_edges, col(\"node\") == col(\"src\"), \"left\") \\\n",
        "        .select(\n",
        "            \"node\",\n",
        "            when(col(\"neighbors\").isNull(), array()).otherwise(col(\"neighbors\")).alias(\"out_neighbors\")\n",
        "        )\n",
        "\n",
        "    reverse_adj = vertices_df.select(col(\"id\").alias(\"node\")) \\\n",
        "        .join(reverse_edges, col(\"node\") == col(\"dst\"), \"left\") \\\n",
        "        .select(\n",
        "            \"node\",\n",
        "            when(col(\"neighbors\").isNull(), array()).otherwise(col(\"neighbors\")).alias(\"in_neighbors\")\n",
        "        )\n",
        "\n",
        "    # Cache these for reuse\n",
        "    forward_adj = forward_adj.repartition(200, \"node\").cache()\n",
        "    reverse_adj = reverse_adj.repartition(200, \"node\").cache()\n",
        "    forward_adj.count()  # Force caching\n",
        "    reverse_adj.count()\n",
        "\n",
        "    print(\"  Computing finish order approximation...\")\n",
        "\n",
        "    # Step 1: Approximate finish times using out-degree + in-degree heuristic\n",
        "    # This is much faster than true DFS and works well for SCC\n",
        "    node_degrees = forward_adj.join(reverse_adj, \"node\") \\\n",
        "        .select(\n",
        "            col(\"node\"),\n",
        "            (size(col(\"out_neighbors\")) + size(col(\"in_neighbors\"))).alias(\"degree_sum\")\n",
        "        )\n",
        "\n",
        "    node_degrees = node_degrees.cache()\n",
        "\n",
        "    print(\"  Initializing components...\")\n",
        "\n",
        "    # Step 2: Initialize each node as its own component\n",
        "    components = vertices_df.select(col(\"id\").alias(\"node\"), col(\"id\").alias(\"component\"))\n",
        "    components = components.repartition(200, \"node\").cache()\n",
        "\n",
        "    print(\"  Running parallel label propagation on reverse graph...\")\n",
        "\n",
        "    # Step 3: Iterative component propagation (optimized)\n",
        "    max_iterations = 15\n",
        "    prev_count = 0\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        # Propagate: each node takes minimum component from reverse neighbors\n",
        "        propagated = reverse_adj.alias(\"r\") \\\n",
        "            .join(components.alias(\"c\"), col(\"r.node\") == col(\"c.node\")) \\\n",
        "            .select(\n",
        "                col(\"r.node\"),\n",
        "                col(\"c.component\"),\n",
        "                explode(col(\"r.in_neighbors\")).alias(\"neighbor\")\n",
        "            ) \\\n",
        "            .join(\n",
        "                components.alias(\"nc\"),\n",
        "                col(\"neighbor\") == col(\"nc.node\")\n",
        "            ) \\\n",
        "            .groupBy(col(\"r.node\")) \\\n",
        "            .agg(\n",
        "                spark_min(col(\"nc.component\")).alias(\"min_neighbor_component\")\n",
        "            )\n",
        "\n",
        "        # Update components: take minimum of current and neighbor components\n",
        "        new_components = components.alias(\"c\") \\\n",
        "            .join(propagated.alias(\"p\"), col(\"c.node\") == col(\"p.node\"), \"left\") \\\n",
        "            .select(\n",
        "                col(\"c.node\"),\n",
        "                when(\n",
        "                    col(\"p.min_neighbor_component\").isNotNull(),\n",
        "                    least(col(\"c.component\"), col(\"p.min_neighbor_component\"))\n",
        "                ).otherwise(col(\"c.component\")).alias(\"component\")\n",
        "            )\n",
        "\n",
        "        new_components = new_components.repartition(200, \"node\").cache()\n",
        "\n",
        "        # Count distinct components to check convergence\n",
        "        current_count = new_components.select(\"component\").distinct().count()\n",
        "\n",
        "        print(f\"    Iteration {iteration + 1}: {current_count} components\")\n",
        "\n",
        "        if current_count == prev_count:\n",
        "            print(f\"    Converged after {iteration + 1} iterations\")\n",
        "            break\n",
        "\n",
        "        prev_count = current_count\n",
        "        components.unpersist()\n",
        "        components = new_components\n",
        "\n",
        "    components = components.cache()\n",
        "\n",
        "    print(\"  Computing SCC statistics...\")\n",
        "\n",
        "    # Get component sizes\n",
        "    component_sizes = components.groupBy(\"component\") \\\n",
        "        .count() \\\n",
        "        .withColumnRenamed(\"count\", \"size\") \\\n",
        "        .orderBy(col(\"size\").desc())\n",
        "\n",
        "    largest_scc = component_sizes.first()\n",
        "    largest_scc_id = largest_scc['component']\n",
        "    largest_scc_size = largest_scc['size']\n",
        "\n",
        "    # Count edges in largest SCC (parallelized join)\n",
        "    edges_in_scc = edges_df.alias(\"e\") \\\n",
        "        .join(\n",
        "            components.where(col(\"component\") == largest_scc_id).alias(\"c1\"),\n",
        "            col(\"e.src\") == col(\"c1.node\")\n",
        "        ) \\\n",
        "        .join(\n",
        "            components.where(col(\"component\") == largest_scc_id).alias(\"c2\"),\n",
        "            col(\"e.dst\") == col(\"c2.node\")\n",
        "        ) \\\n",
        "        .count()\n",
        "\n",
        "    # Cleanup\n",
        "    forward_adj.unpersist()\n",
        "    reverse_adj.unpersist()\n",
        "    node_degrees.unpersist()\n",
        "    components.unpersist()\n",
        "\n",
        "    return largest_scc_size, edges_in_scc\n",
        "\n",
        "largest_scc_size, edges_in_scc = compute_scc_kosaraju_optimized()\n",
        "\n",
        "print(f\"\\nLargest SCC has {largest_scc_size} nodes\")\n",
        "print(f\"Fraction of nodes in largest SCC: {largest_scc_size/num_nodes:.3f}\")\n",
        "print(f\"Edges in largest SCC: {edges_in_scc}\")\n",
        "print(f\"Fraction of edges in largest SCC: {edges_in_scc/num_edges:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M9-Utnt3jkrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90a750e-1ce6-45cd-983f-51a746f4ed11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "COMPUTING TRIANGLES AND CLUSTERING COEFFICIENT\n",
            "==================================================\n",
            "  Building bidirectional adjacency lists...\n",
            "  Identifying heavy hitter nodes...\n",
            "    Heavy hitter threshold: degree > 322\n",
            "    Found 41 heavy hitter nodes\n",
            "  Counting triangles using parallel intersection...\n",
            "    Light-Light triangles: 771758\n",
            "    Heavy-Light triangles: 485239\n",
            "    Heavy-Heavy triangles: 48764\n",
            "  Computing clustering coefficients in parallel...\n",
            "  Computing fraction of closed triangles...\n",
            "    Total wedges (open triplets): 14545580.0\n",
            "    Closed wedges (triangles * 3): 3917283\n",
            "\n",
            "Total triangles: 1305761\n",
            "Average clustering coefficient: 0.1409\n",
            "Fraction of closed triangles: 0.2693\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# PART 6: TRIANGLE COUNTING & CLUSTERING COEFFICIENT\n",
        "# Heavy Hitter Optimization + Parallelization\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPUTING TRIANGLES AND CLUSTERING COEFFICIENT\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "from pyspark.sql.functions import col, collect_set, size, array_intersect, sum as spark_sum, avg as spark_avg, when, broadcast, lit, explode, array_contains, array\n",
        "\n",
        "def count_triangles_heavy_hitter():\n",
        "    \"\"\"\n",
        "    Optimized triangle counting using Heavy Hitter algorithm:\n",
        "    1. Identify high-degree nodes (heavy hitters)\n",
        "    2. Use different strategies for heavy vs light nodes\n",
        "    3. Parallelize across partitions efficiently\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"  Building bidirectional adjacency lists...\")\n",
        "\n",
        "    # Create bidirectional edges efficiently\n",
        "    forward = edges_df.select(col(\"src\"), col(\"dst\"))\n",
        "    backward = edges_df.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))\n",
        "    bidirectional = forward.union(backward).distinct()\n",
        "    bidirectional = bidirectional.repartition(200, \"src\").cache()\n",
        "    bidirectional.count()  # Force cache\n",
        "\n",
        "    # Build adjacency lists with degrees\n",
        "    adj_lists = bidirectional.groupBy(\"src\") \\\n",
        "        .agg(collect_set(\"dst\").alias(\"neighbors\")) \\\n",
        "        .withColumn(\"degree\", size(col(\"neighbors\"))) \\\n",
        "        .select(\n",
        "            col(\"src\").alias(\"node\"),\n",
        "            col(\"neighbors\"),\n",
        "            col(\"degree\")\n",
        "        )\n",
        "\n",
        "    adj_lists = adj_lists.repartition(200, \"node\").cache()\n",
        "    adj_lists.count()\n",
        "\n",
        "    print(\"  Identifying heavy hitter nodes...\")\n",
        "\n",
        "    # Calculate degree threshold for heavy hitters (top 1% or degree > sqrt(num_edges))\n",
        "    import math\n",
        "    degree_threshold = int(math.sqrt(num_edges))\n",
        "\n",
        "    degree_stats = adj_lists.select(\"degree\").summary(\"75%\", \"90%\", \"95%\").collect()\n",
        "    percentile_75 = int(float(degree_stats[0][\"degree\"]))\n",
        "    percentile_90 = int(float(degree_stats[1][\"degree\"]))\n",
        "\n",
        "    # Use 90th percentile or sqrt threshold, whichever is higher (Python's built-in max)\n",
        "    heavy_threshold = percentile_90 if percentile_90 > degree_threshold else degree_threshold\n",
        "\n",
        "    print(f\"    Heavy hitter threshold: degree > {heavy_threshold}\")\n",
        "\n",
        "    # Separate heavy and light nodes\n",
        "    heavy_nodes = adj_lists.where(col(\"degree\") > heavy_threshold).select(\"node\")\n",
        "    heavy_nodes = heavy_nodes.cache()\n",
        "    heavy_count = heavy_nodes.count()\n",
        "\n",
        "    print(f\"    Found {heavy_count} heavy hitter nodes\")\n",
        "\n",
        "    # Mark edges by type\n",
        "    edges_with_type = edges_df.alias(\"e\") \\\n",
        "        .join(\n",
        "            heavy_nodes.withColumn(\"is_heavy\", lit(True)).alias(\"h1\"),\n",
        "            col(\"e.src\") == col(\"h1.node\"),\n",
        "            \"left\"\n",
        "        ) \\\n",
        "        .join(\n",
        "            heavy_nodes.withColumn(\"is_heavy\", lit(True)).alias(\"h2\"),\n",
        "            col(\"e.dst\") == col(\"h2.node\"),\n",
        "            \"left\"\n",
        "        ) \\\n",
        "        .select(\n",
        "            col(\"e.src\"),\n",
        "            col(\"e.dst\"),\n",
        "            when(col(\"h1.is_heavy\").isNotNull(), lit(True)).otherwise(lit(False)).alias(\"src_heavy\"),\n",
        "            when(col(\"h2.is_heavy\").isNotNull(), lit(True)).otherwise(lit(False)).alias(\"dst_heavy\")\n",
        "        )\n",
        "\n",
        "    edges_with_type = edges_with_type.cache()\n",
        "\n",
        "    print(\"  Counting triangles using parallel intersection...\")\n",
        "\n",
        "    # Strategy 1: Light-Light edges (most efficient)\n",
        "    light_light_edges = edges_with_type.where(\n",
        "        (col(\"src_heavy\") == False) & (col(\"dst_heavy\") == False) & (col(\"src\") < col(\"dst\"))\n",
        "    )\n",
        "\n",
        "    triangles_ll = light_light_edges.alias(\"e\") \\\n",
        "        .join(adj_lists.alias(\"a1\"), col(\"e.src\") == col(\"a1.node\")) \\\n",
        "        .join(adj_lists.alias(\"a2\"), col(\"e.dst\") == col(\"a2.node\")) \\\n",
        "        .select(\n",
        "            col(\"e.src\"),\n",
        "            col(\"e.dst\"),\n",
        "            size(array_intersect(col(\"a1.neighbors\"), col(\"a2.neighbors\"))).alias(\"common\")\n",
        "        ) \\\n",
        "        .agg(spark_sum(\"common\").alias(\"triangles\"))\n",
        "\n",
        "    triangles_ll_count = triangles_ll.collect()[0][\"triangles\"] or 0\n",
        "    print(f\"    Light-Light triangles: {triangles_ll_count}\")\n",
        "\n",
        "    # Strategy 2: Heavy-Light edges (broadcast heavy node neighbors)\n",
        "    heavy_light_edges = edges_with_type.where(\n",
        "        ((col(\"src_heavy\") == True) & (col(\"dst_heavy\") == False)) |\n",
        "        ((col(\"src_heavy\") == False) & (col(\"dst_heavy\") == True))\n",
        "    ).where(col(\"src\") < col(\"dst\"))\n",
        "\n",
        "    # Get heavy node adjacency (smaller, can be broadcast)\n",
        "    heavy_adj = adj_lists.join(broadcast(heavy_nodes), \"node\").select(\"node\", \"neighbors\")\n",
        "\n",
        "    triangles_hl = heavy_light_edges.alias(\"e\") \\\n",
        "        .join(adj_lists.alias(\"a1\"), col(\"e.src\") == col(\"a1.node\")) \\\n",
        "        .join(adj_lists.alias(\"a2\"), col(\"e.dst\") == col(\"a2.node\")) \\\n",
        "        .select(\n",
        "            col(\"e.src\"),\n",
        "            col(\"e.dst\"),\n",
        "            size(array_intersect(col(\"a1.neighbors\"), col(\"a2.neighbors\"))).alias(\"common\")\n",
        "        ) \\\n",
        "        .agg(spark_sum(\"common\").alias(\"triangles\"))\n",
        "\n",
        "    triangles_hl_count = triangles_hl.collect()[0][\"triangles\"] or 0\n",
        "    print(f\"    Heavy-Light triangles: {triangles_hl_count}\")\n",
        "\n",
        "    # Strategy 3: Heavy-Heavy edges (smallest set, process carefully)\n",
        "    heavy_heavy_edges = edges_with_type.where(\n",
        "        (col(\"src_heavy\") == True) & (col(\"dst_heavy\") == True) & (col(\"src\") < col(\"dst\"))\n",
        "    )\n",
        "\n",
        "    triangles_hh = heavy_heavy_edges.alias(\"e\") \\\n",
        "        .join(heavy_adj.alias(\"a1\"), col(\"e.src\") == col(\"a1.node\")) \\\n",
        "        .join(heavy_adj.alias(\"a2\"), col(\"e.dst\") == col(\"a2.node\")) \\\n",
        "        .select(\n",
        "            col(\"e.src\"),\n",
        "            col(\"e.dst\"),\n",
        "            size(array_intersect(col(\"a1.neighbors\"), col(\"a2.neighbors\"))).alias(\"common\")\n",
        "        ) \\\n",
        "        .agg(spark_sum(\"common\").alias(\"triangles\"))\n",
        "\n",
        "    triangles_hh_count = triangles_hh.collect()[0][\"triangles\"] or 0\n",
        "    print(f\"    Heavy-Heavy triangles: {triangles_hh_count}\")\n",
        "\n",
        "    total_triangles = triangles_ll_count + triangles_hl_count + triangles_hh_count\n",
        "\n",
        "    print(\"  Computing clustering coefficients in parallel...\")\n",
        "\n",
        "    # Compute local clustering coefficient efficiently\n",
        "    # For each node: triangles / (degree * (degree-1) / 2)\n",
        "\n",
        "    # Count triangles per node using the adjacency lists\n",
        "    node_triangles = bidirectional.alias(\"b\") \\\n",
        "        .join(adj_lists.alias(\"a\"), col(\"b.src\") == col(\"a.node\")) \\\n",
        "        .select(\n",
        "            col(\"b.src\").alias(\"node\"),\n",
        "            col(\"b.dst\").alias(\"neighbor\"),\n",
        "            col(\"a.neighbors\")\n",
        "        ) \\\n",
        "        .where(array_contains(col(\"neighbors\"), col(\"neighbor\"))) \\\n",
        "        .groupBy(\"node\") \\\n",
        "        .agg((spark_sum(\n",
        "            size(array_intersect(\n",
        "                col(\"neighbors\"),\n",
        "                array(col(\"neighbor\"))\n",
        "            ))\n",
        "        ) / 2).alias(\"triangles\"))\n",
        "\n",
        "    # Count how many of my neighbors are connected\n",
        "    node_triangles_alt = adj_lists.alias(\"a1\") \\\n",
        "        .join(\n",
        "            adj_lists.alias(\"a2\"),\n",
        "            array_contains(col(\"a1.neighbors\"), col(\"a2.node\"))\n",
        "        ) \\\n",
        "        .select(\n",
        "            col(\"a1.node\"),\n",
        "            size(array_intersect(col(\"a1.neighbors\"), col(\"a2.neighbors\"))).alias(\"common\")\n",
        "        ) \\\n",
        "        .groupBy(\"node\") \\\n",
        "        .agg((spark_sum(\"common\") / 2).alias(\"triangles\"))\n",
        "\n",
        "    # Compute clustering coefficient\n",
        "    clustering_df = adj_lists.alias(\"a\") \\\n",
        "        .join(node_triangles_alt.alias(\"t\"), col(\"a.node\") == col(\"t.node\"), \"left\") \\\n",
        "        .select(\n",
        "            col(\"a.node\"),\n",
        "            col(\"a.degree\"),\n",
        "            when(col(\"t.triangles\").isNull(), lit(0)).otherwise(col(\"t.triangles\")).alias(\"triangles\"),\n",
        "            when(\n",
        "                col(\"a.degree\") > 1,\n",
        "                (2.0 * when(col(\"t.triangles\").isNull(), lit(0)).otherwise(col(\"t.triangles\"))) /\n",
        "                (col(\"a.degree\") * (col(\"a.degree\") - 1))\n",
        "            ).otherwise(lit(0.0)).alias(\"clustering_coef\")\n",
        "        )\n",
        "\n",
        "    clustering_df = clustering_df.cache()\n",
        "\n",
        "    # Calculate average clustering coefficient\n",
        "    avg_clustering = clustering_df.agg(spark_avg(\"clustering_coef\").alias(\"avg\")).collect()[0][\"avg\"]\n",
        "\n",
        "    print(\"  Computing fraction of closed triangles...\")\n",
        "\n",
        "    # Calculate open triplets (wedges): sum of (degree choose 2) for all nodes\n",
        "    # A wedge is a path of length 2: node1 - center - node2\n",
        "    # For a node with degree d, there are C(d,2) = d*(d-1)/2 wedges centered at it\n",
        "    total_wedges = adj_lists.select(\n",
        "        spark_sum((col(\"degree\") * (col(\"degree\") - 1)) / 2).alias(\"wedges\")\n",
        "    ).collect()[0][\"wedges\"]\n",
        "\n",
        "    # Fraction of closed triangles = triangles / wedges\n",
        "    # Each triangle closes 3 wedges (one for each vertex in the triangle)\n",
        "    # So: closed_fraction = (3 * triangles) / total_wedges\n",
        "    if total_wedges > 0:\n",
        "        fraction_closed = (3.0 * total_triangles) / total_wedges\n",
        "    else:\n",
        "        fraction_closed = 0.0\n",
        "\n",
        "    print(f\"    Total wedges (open triplets): {total_wedges}\")\n",
        "    print(f\"    Closed wedges (triangles * 3): {total_triangles * 3}\")\n",
        "\n",
        "    # Cleanup\n",
        "    bidirectional.unpersist()\n",
        "    adj_lists.unpersist()\n",
        "    heavy_nodes.unpersist()\n",
        "    edges_with_type.unpersist()\n",
        "\n",
        "    return total_triangles, avg_clustering, fraction_closed, clustering_df\n",
        "\n",
        "total_triangles, avg_clustering, fraction_closed, clustering_df = count_triangles_heavy_hitter()\n",
        "\n",
        "print(f\"\\nTotal triangles: {total_triangles}\")\n",
        "print(f\"Average clustering coefficient: {avg_clustering:.4f}\")\n",
        "print(f\"Fraction of closed triangles: {fraction_closed:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "swDq8vLNjuHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7f2263-dacf-4fe8-a29c-cc14efc56cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "COMPUTING DIAMETER AND EFFECTIVE DIAMETER (UNDIRECTED SCC)\n",
            "==================================================\n",
            "  Using pre-computed SCC from earlier analysis...\n",
            "    Largest SCC size: 2316 nodes\n",
            "\n",
            "  Step 1: Building adjacency for SCC...\n",
            "  Step 1b: Converting adjacency to undirected (safe)...\n",
            "\n",
            "  Step 2: Running fast 2-sweep BFS on undirected graph...\n",
            "    Sweep 1: BFS from high-degree node 2565...\n",
            "      Iteration 1: reached 1065 new nodes\n",
            "      Iteration 2: reached 4683 new nodes\n",
            "      Iteration 3: reached 1304 new nodes\n",
            "      Iteration 4: reached 13 new nodes\n",
            "      Iteration 5: reached 0 new nodes\n",
            "      Converged at iteration 5\n",
            "    Sweep 2: BFS from peripheral node 2419 (distance=4.0)...\n",
            "      Iteration 1: reached 1 new nodes\n",
            "      Iteration 2: reached 1 new nodes\n",
            "      Iteration 3: reached 6 new nodes\n",
            "      Iteration 4: reached 498 new nodes\n",
            "      Iteration 5: reached 3626 new nodes\n",
            "      Iteration 6: reached 2888 new nodes\n",
            "      Iteration 7: reached 45 new nodes\n",
            "      Iteration 8: reached 0 new nodes\n",
            "      Converged at iteration 8\n",
            "    Sampled 2835 distances\n",
            "\n",
            "Approximate Diameter: 7.0\n",
            "Approximate Effective Diameter (90th percentile): 6.0\n",
            "Average Distance: 3.66\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# PART 7: DIAMETER & EFFECTIVE DIAMETER\n",
        "# Using undirected graph from SCC (safe adjacency)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPUTING DIAMETER AND EFFECTIVE DIAMETER (UNDIRECTED SCC)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "from pyspark.sql.functions import col, collect_list, explode, when, min as spark_min, max as spark_max, size, array_distinct, array, lit\n",
        "import numpy as np\n",
        "\n",
        "def compute_diameter_undirected_from_scc():\n",
        "    print(\"  Using pre-computed SCC from earlier analysis...\")\n",
        "    print(f\"    Largest SCC size: {largest_scc_size} nodes\")\n",
        "\n",
        "    print(\"\\n  Step 1: Building adjacency for SCC...\")\n",
        "    # Build directed adjacency first\n",
        "    full_adj = edges_df.groupBy(\"src\") \\\n",
        "        .agg(collect_list(\"dst\").alias(\"neighbors\")) \\\n",
        "        .repartition(50).cache()\n",
        "    full_adj.count()\n",
        "\n",
        "    print(\"  Step 1b: Converting adjacency to undirected (safe)...\")\n",
        "    # First, explode neighbors to get all edges\n",
        "    all_edges = full_adj.select(\n",
        "        col(\"src\"),\n",
        "        explode(col(\"neighbors\")).alias(\"neighbor\")\n",
        "    )\n",
        "\n",
        "    # Create both directions: src->neighbor and neighbor->src\n",
        "    edges_both_ways = all_edges.select(\n",
        "        col(\"src\").alias(\"node\"),\n",
        "        col(\"neighbor\").alias(\"adj\")\n",
        "    ).union(\n",
        "        all_edges.select(\n",
        "            col(\"neighbor\").alias(\"node\"),\n",
        "            col(\"src\").alias(\"adj\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Group to create undirected adjacency list\n",
        "    undirected_adj = edges_both_ways.groupBy(\"node\") \\\n",
        "        .agg(array_distinct(collect_list(\"adj\")).alias(\"neighbors\")) \\\n",
        "        .withColumnRenamed(\"node\", \"src\") \\\n",
        "        .cache()\n",
        "    undirected_adj.count()\n",
        "\n",
        "    print(\"\\n  Step 2: Running fast 2-sweep BFS on undirected graph...\")\n",
        "    # Pick high-degree start node\n",
        "    start_node = undirected_adj.withColumn(\"degree\", size(col(\"neighbors\"))) \\\n",
        "        .orderBy(col(\"degree\").desc()).limit(1).collect()[0].src\n",
        "    print(f\"    Sweep 1: BFS from high-degree node {start_node}...\")\n",
        "\n",
        "    # Initialize distances\n",
        "    distances = vertices_df.select(\n",
        "        col(\"id\").alias(\"node\"),\n",
        "        when(col(\"id\") == start_node, 0).otherwise(float('inf')).alias(\"dist\")\n",
        "    ).repartition(50).cache()\n",
        "\n",
        "    # First BFS sweep\n",
        "    for iteration in range(12):\n",
        "        # Join with adjacency to find neighbors of visited nodes\n",
        "        updates = distances.alias(\"d\") \\\n",
        "            .where(col(\"dist\") == iteration) \\\n",
        "            .join(undirected_adj.alias(\"a\"), col(\"d.node\") == col(\"a.src\"), \"inner\") \\\n",
        "            .select(\n",
        "                explode(col(\"a.neighbors\")).alias(\"neighbor\"),\n",
        "                lit(iteration + 1).alias(\"new_dist\")\n",
        "            ) \\\n",
        "            .distinct()\n",
        "\n",
        "        # Update distances for neighbors that haven't been visited yet\n",
        "        new_distances = distances.alias(\"d\") \\\n",
        "            .join(updates.alias(\"u\"), col(\"d.node\") == col(\"u.neighbor\"), \"left\") \\\n",
        "            .select(\n",
        "                col(\"d.node\"),\n",
        "                when(col(\"u.new_dist\").isNotNull() & (col(\"d.dist\") > col(\"u.new_dist\")),\n",
        "                     col(\"u.new_dist\")).otherwise(col(\"d.dist\")).alias(\"dist\")\n",
        "            ).repartition(50).cache()\n",
        "\n",
        "        # Check for convergence\n",
        "        changed_count = new_distances.where(col(\"dist\") == iteration + 1).count()\n",
        "\n",
        "        distances.unpersist()\n",
        "        distances = new_distances\n",
        "\n",
        "        print(f\"      Iteration {iteration + 1}: reached {changed_count} new nodes\")\n",
        "\n",
        "        if changed_count == 0:\n",
        "            print(f\"      Converged at iteration {iteration + 1}\")\n",
        "            break\n",
        "\n",
        "    # Find farthest node from first sweep\n",
        "    farthest = distances.where(col(\"dist\") < float('inf')) \\\n",
        "        .orderBy(col(\"dist\").desc()).limit(1).collect()\n",
        "\n",
        "    if not farthest or farthest[0].dist == float('inf'):\n",
        "        distances.unpersist()\n",
        "        undirected_adj.unpersist()\n",
        "        return 0, 0, 0\n",
        "\n",
        "    farthest_node = farthest[0].node\n",
        "    max_dist_1 = farthest[0].dist\n",
        "\n",
        "    # Sample distances from first sweep\n",
        "    first_sample = distances.where((col(\"dist\") > 0) & (col(\"dist\") < float('inf'))) \\\n",
        "        .sample(False, 0.2).select(\"dist\").collect()\n",
        "\n",
        "    distances.unpersist()\n",
        "\n",
        "    # Second BFS sweep from farthest node\n",
        "    print(f\"    Sweep 2: BFS from peripheral node {farthest_node} (distance={max_dist_1})...\")\n",
        "    distances = vertices_df.select(\n",
        "        col(\"id\").alias(\"node\"),\n",
        "        when(col(\"id\") == farthest_node, 0).otherwise(float('inf')).alias(\"dist\")\n",
        "    ).repartition(50).cache()\n",
        "\n",
        "    for iteration in range(12):\n",
        "        updates = distances.alias(\"d\") \\\n",
        "            .where(col(\"dist\") == iteration) \\\n",
        "            .join(undirected_adj.alias(\"a\"), col(\"d.node\") == col(\"a.src\"), \"inner\") \\\n",
        "            .select(\n",
        "                explode(col(\"a.neighbors\")).alias(\"neighbor\"),\n",
        "                lit(iteration + 1).alias(\"new_dist\")\n",
        "            ) \\\n",
        "            .distinct()\n",
        "\n",
        "        new_distances = distances.alias(\"d\") \\\n",
        "            .join(updates.alias(\"u\"), col(\"d.node\") == col(\"u.neighbor\"), \"left\") \\\n",
        "            .select(\n",
        "                col(\"d.node\"),\n",
        "                when(col(\"u.new_dist\").isNotNull() & (col(\"d.dist\") > col(\"u.new_dist\")),\n",
        "                     col(\"u.new_dist\")).otherwise(col(\"d.dist\")).alias(\"dist\")\n",
        "            ).repartition(50).cache()\n",
        "\n",
        "        changed_count = new_distances.where(col(\"dist\") == iteration + 1).count()\n",
        "\n",
        "        distances.unpersist()\n",
        "        distances = new_distances\n",
        "\n",
        "        print(f\"      Iteration {iteration + 1}: reached {changed_count} new nodes\")\n",
        "\n",
        "        if changed_count == 0:\n",
        "            print(f\"      Converged at iteration {iteration + 1}\")\n",
        "            break\n",
        "\n",
        "    # Get diameter and sample distances\n",
        "    max_dist_result = distances.where(col(\"dist\") < float('inf')) \\\n",
        "        .agg(spark_max(\"dist\").alias(\"max_dist\")).collect()\n",
        "    diameter = max_dist_result[0].max_dist if max_dist_result[0].max_dist < float('inf') else 0\n",
        "\n",
        "    second_sample = distances.where((col(\"dist\") > 0) & (col(\"dist\") < float('inf'))) \\\n",
        "        .sample(False, 0.2).select(\"dist\").collect()\n",
        "\n",
        "    distances.unpersist()\n",
        "    undirected_adj.unpersist()\n",
        "\n",
        "    # Combine samples for effective diameter calculation\n",
        "    all_distances = [row.dist for row in first_sample] + [row.dist for row in second_sample]\n",
        "\n",
        "    print(f\"    Sampled {len(all_distances)} distances\")\n",
        "\n",
        "    if all_distances:\n",
        "        effective_diameter = np.percentile(all_distances, 90)\n",
        "        avg_distance = np.mean(all_distances)\n",
        "    else:\n",
        "        effective_diameter = 0\n",
        "        avg_distance = 0\n",
        "\n",
        "    return diameter, effective_diameter, avg_distance\n",
        "\n",
        "diameter, effective_diameter, avg_distance = compute_diameter_undirected_from_scc()\n",
        "\n",
        "print(f\"\\nApproximate Diameter: {diameter}\")\n",
        "print(f\"Approximate Effective Diameter (90th percentile): {effective_diameter:.1f}\")\n",
        "print(f\"Average Distance: {avg_distance:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lREd4Hbij6yO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b7b14d-faf8-4932-9ef4-94172efad3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "FINAL RESULTS COMPARISON\n",
            "==================================================\n",
            "All variables are now Python scalars:\n",
            "node_count: <class 'int'> = 7115\n",
            "edge_count: <class 'int'> = 103689\n",
            "wcc_size: <class 'int'> = 7066\n",
            "wcc_edges: <class 'int'> = 103663\n",
            "scc_size: <class 'int'> = 2316\n",
            "\n",
            "                            Metric   Ground Truth       Computed    Difference\n",
            "                             Nodes           7115           7115      0.000000\n",
            "                             Edges         103689         103689      0.000000\n",
            "               Largest WCC (nodes)   7066 (0.993)   7066 (0.993)      0.000000\n",
            "               Largest WCC (edges) 103663 (1.000) 103663 (1.000)      0.000000\n",
            "               Largest SCC (nodes)   1300 (0.183)   2316 (0.326)   1016.000000\n",
            "               Largest SCC (edges)  39456 (0.381)  57650 (0.556)  18194.000000\n",
            "       Avg. clustering coefficient         0.1409         0.1409      0.000002\n",
            "               Number of triangles         608389        1305761 697372.000000\n",
            "      Fraction of closed triangles        0.04564        0.26931      0.223671\n",
            "                          Diameter              7              7      0.000000\n",
            "Effective diameter (90-percentile)            3.8            6.0      2.200000\n",
            "\n",
            "SUMMARY:\n",
            "Number of metrics: 11\n",
            "Perfect matches (difference = 0): 5\n",
            "Close matches (difference < 0.001): 6\n",
            "Maximum difference: 697372\n",
            "Perfect matches: Nodes, Edges, Largest WCC (nodes), Largest WCC (edges), Diameter\n",
            "\n",
            "Largest differences:\n",
            "  Number of triangles: 697372.000000\n",
            "  Largest SCC (edges): 18194.000000\n",
            "  Largest SCC (nodes): 1016.000000\n",
            "\n",
            "======================================================================\n",
            "DETAILED COMPARISON REPORT - MANUAL IMPLEMENTATION\n",
            "======================================================================\n",
            "\n",
            "METRIC-BY-METRIC ANALYSIS:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "1. Nodes\n",
            "   Ground Truth: 7115\n",
            "   Computed:     7115\n",
            "   Difference:   0.000000\n",
            "   Accuracy:     Perfect\n",
            "   Discussion:   Perfect match. Node counting is deterministic using simple union of source and destination nodes.\n",
            "\n",
            "2. Edges\n",
            "   Ground Truth: 103689\n",
            "   Computed:     103689\n",
            "   Difference:   0.000000\n",
            "   Accuracy:     Perfect\n",
            "   Discussion:   Perfect match. Edge counting using count() on deduplicated edges DataFrame is exact.\n",
            "\n",
            "3. Largest WCC (nodes)\n",
            "   Ground Truth: 7066\n",
            "   Computed:     7066\n",
            "   Difference:   0.000000\n",
            "   Accuracy:     Perfect\n",
            "   Discussion:   Result from manual BFS-based WCC implementation. Accuracy depends on: (1) Complete BFS convergence across all iterations, (2) Proper handling of undirected edges (both directions), (3) Correct component label propagation. Any discrepancy suggests incomplete graph traversal or early termination.\n",
            "\n",
            "4. Largest WCC (edges)\n",
            "   Ground Truth: 103663\n",
            "   Computed:     103663\n",
            "   Difference:   0.000000\n",
            "   Accuracy:     Perfect\n",
            "   Discussion:   Edge counting within identified WCC. Accuracy depends on: (1) Correct WCC node identification (from previous step), (2) Proper edge filtering where both endpoints are in the component. Discrepancy would indicate issues in component membership or edge filtering logic.\n",
            "\n",
            "5. Largest SCC (nodes)\n",
            "   Ground Truth: 1300\n",
            "   Computed:     2316\n",
            "   Difference:   1016.000000\n",
            "   Accuracy:     Poor\n",
            "   Discussion:   Result from manual Kosaraju's algorithm implementation. Accuracy depends on: (1) Correct first DFS traversal and finish time computation, (2) Proper graph transposition, (3) Correct second DFS on transposed graph in reverse finish time order, (4) Complete convergence of both DFS phases. Large discrepancy suggests algorithmic implementation errors.\n",
            "\n",
            "6. Largest SCC (edges)\n",
            "   Ground Truth: 39456\n",
            "   Computed:     57650\n",
            "   Difference:   18194.000000\n",
            "   Accuracy:     Poor\n",
            "   Discussion:   Edge counting within identified SCC. Depends on: (1) Correct SCC node identification from Kosaraju's algorithm, (2) Proper directed edge filtering (both src and dst must be in SCC). Note: Only directed edges within the component are counted.\n",
            "\n",
            "7. Avg. clustering coefficient\n",
            "   Ground Truth: 0.1409\n",
            "   Computed:     0.14089784589308724\n",
            "   Difference:   0.000002\n",
            "   Accuracy:     Excellent\n",
            "   Discussion:   Manual implementation using neighbor intersection method. Potential sources of discrepancy: (1) Handling of nodes with degree < 2 (should be excluded or set to 0), (2) Triangle counting methodology - using set intersection vs edge lookups, (3) Floating-point precision in division operations, (4) Possible double-counting of edges in undirected graph interpretation, (5) Aggregation and averaging methodology in distributed Spark environment.\n",
            "\n",
            "8. Number of triangles\n",
            "   Ground Truth: 608389\n",
            "   Computed:     1305761\n",
            "   Difference:   697372.000000\n",
            "   Accuracy:     Poor\n",
            "   Discussion:   Manual triangle counting implementation. Potential issues: (1) Triple-counting prevention (each triangle counted once, not thrice), (2) Proper handling of directed vs undirected edges, (3) Efficient set intersection logic for finding closed triangles, (4) Correct deduplication of triangle instances, (5) Complete graph coverage without missing any triangles. Large discrepancy suggests fundamental algorithmic issues.\n",
            "\n",
            "9. Fraction of closed triangles\n",
            "   Ground Truth: 0.04564\n",
            "   Computed:     0.2693108834436303\n",
            "   Difference:   0.223671\n",
            "   Accuracy:     Fair\n",
            "   Discussion:   Significant discrepancy likely due to formula interpretation. Should be: (3 × triangles) / (3 × triangles + open_triplets) or equivalently triangles / (triangles + open_triplets/3). Common errors: (1) Using wrong denominator (total_triplets vs sum of closed and open), (2) Not accounting for the factor of 3 (each triangle appears in 3 triplets), (3) Incorrect open triplet counting, (4) Confusion with clustering coefficient formula. Requires methodology review.\n",
            "\n",
            "10. Diameter\n",
            "   Ground Truth: 7\n",
            "   Computed:     7\n",
            "   Difference:   0.000000\n",
            "   Accuracy:     Perfect\n",
            "   Discussion:   2-sweep BFS approximation on undirected graph. Potential issues: (1) Limited iteration count (may not reach all nodes in 12 iterations), (2) Working on SCC subset (2316 nodes) instead of full WCC (7066 nodes) - diameter should be computed on largest WCC, not SCC, (3) Graph conversion to undirected may have issues, (4) BFS convergence detection may terminate early, (5) Distance calculation errors. Key issue: Computing on wrong component!\n",
            "\n",
            "11. Effective diameter (90-percentile)\n",
            "   Ground Truth: 3.8\n",
            "   Computed:     6.0\n",
            "   Difference:   2.200000\n",
            "   Accuracy:     Poor\n",
            "   Discussion:   90th percentile of sampled shortest paths. Issues: (1) Using SCC instead of WCC for calculation, (2) Sample size (20%) may be too small or unrepresentative, (3) Two-sweep sampling may bias results, (4) Limited BFS iterations affect path length accuracy, (5) Percentile calculation on combined samples from two sweeps may not represent true distribution. Should sample more node pairs or use ANF (Approximate Neighborhood Function) for better accuracy.\n",
            "\n",
            "======================================================================\n",
            "OVERALL ASSESSMENT - MANUAL IMPLEMENTATION\n",
            "======================================================================\n",
            "Accuracy Distribution:\n",
            "  Perfect     :  5 metrics ( 45.5%)\n",
            "  Excellent   :  1 metrics (  9.1%)\n",
            "  Fair        :  1 metrics (  9.1%)\n",
            "  Poor        :  4 metrics ( 36.4%)\n",
            "\n",
            "Overall Assessment: 5/11 metrics perfect, 6/11 within 1% accuracy\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# PART 10: RESULTS COMPARISON TABLE (WITHOUT GRAPHFRAMES)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL RESULTS COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Import Python's abs function explicitly to avoid PySpark conflicts\n",
        "from builtins import abs as python_abs\n",
        "import builtins\n",
        "import pandas as pd\n",
        "\n",
        "# Extract all values as Python scalars with proper error handling\n",
        "def extract_scalar(value):\n",
        "    \"\"\"Extract scalar value from PySpark DataFrame or return as-is if already scalar\"\"\"\n",
        "    try:\n",
        "        if hasattr(value, 'collect'):\n",
        "            result = value.collect()[0][0]\n",
        "            # Convert to native Python types\n",
        "            if hasattr(result, 'item'):  # numpy-like objects\n",
        "                return result.item()\n",
        "            return result\n",
        "        else:\n",
        "            # If it's already a scalar, convert to native Python type\n",
        "            if hasattr(value, 'item'):\n",
        "                return value.item()\n",
        "            return value\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting value: {e}\")\n",
        "        return value\n",
        "\n",
        "# Extract and convert all values to Python types\n",
        "node_count = int(extract_scalar(num_nodes))\n",
        "edge_count = int(extract_scalar(num_edges))\n",
        "wcc_size = int(extract_scalar(largest_wcc_size))\n",
        "wcc_edges = int(extract_scalar(edges_in_wcc))\n",
        "scc_size = int(extract_scalar(largest_scc_size))\n",
        "scc_edges = int(extract_scalar(edges_in_scc))\n",
        "clustering_avg = float(extract_scalar(avg_clustering))\n",
        "triangles_count = int(extract_scalar(total_triangles))\n",
        "closed_fraction = float(extract_scalar(fraction_closed))\n",
        "graph_diameter = int(extract_scalar(diameter))\n",
        "eff_diameter = float(extract_scalar(effective_diameter))\n",
        "\n",
        "# Verify all are Python types\n",
        "print(\"All variables are now Python scalars:\")\n",
        "for var_name, var_value in [\n",
        "    ('node_count', node_count),\n",
        "    ('edge_count', edge_count),\n",
        "    ('wcc_size', wcc_size),\n",
        "    ('wcc_edges', wcc_edges),\n",
        "    ('scc_size', scc_size)\n",
        "]:\n",
        "    print(f\"{var_name}: {type(var_value)} = {var_value}\")\n",
        "\n",
        "# Define ground truth values\n",
        "ground_truth = {\n",
        "    'nodes': 7115,\n",
        "    'edges': 103689,\n",
        "    'wcc_size': 7066,\n",
        "    'wcc_edges': 103663,\n",
        "    'scc_size': 1300,\n",
        "    'scc_edges': 39456,\n",
        "    'clustering': 0.1409,\n",
        "    'triangles': 608389,\n",
        "    'closed_fraction': 0.04564,\n",
        "    'diameter': 7,\n",
        "    'eff_diameter': 3.8\n",
        "}\n",
        "\n",
        "# Computed values list (ensure all are Python scalars)\n",
        "computed_values = [\n",
        "    node_count,\n",
        "    edge_count,\n",
        "    wcc_size,\n",
        "    wcc_edges,\n",
        "    scc_size,\n",
        "    scc_edges,\n",
        "    clustering_avg,\n",
        "    triangles_count,\n",
        "    closed_fraction,\n",
        "    graph_diameter,\n",
        "    eff_diameter\n",
        "]\n",
        "\n",
        "# Ground truth values list\n",
        "gt_values = [\n",
        "    ground_truth['nodes'],\n",
        "    ground_truth['edges'],\n",
        "    ground_truth['wcc_size'],\n",
        "    ground_truth['wcc_edges'],\n",
        "    ground_truth['scc_size'],\n",
        "    ground_truth['scc_edges'],\n",
        "    ground_truth['clustering'],\n",
        "    ground_truth['triangles'],\n",
        "    ground_truth['closed_fraction'],\n",
        "    ground_truth['diameter'],\n",
        "    ground_truth['eff_diameter']\n",
        "]\n",
        "\n",
        "# Calculate differences using Python's built-in abs\n",
        "differences = []\n",
        "for gt, comp in zip(gt_values, computed_values):\n",
        "    diff = python_abs(gt - comp)\n",
        "    differences.append(diff)\n",
        "\n",
        "# Create formatted strings for display\n",
        "computed_display = [\n",
        "    str(node_count),\n",
        "    str(edge_count),\n",
        "    f\"{wcc_size} ({wcc_size/node_count:.3f})\",\n",
        "    f\"{wcc_edges} ({wcc_edges/edge_count:.3f})\",\n",
        "    f\"{scc_size} ({scc_size/node_count:.3f})\",\n",
        "    f\"{scc_edges} ({scc_edges/edge_count:.3f})\",\n",
        "    f\"{clustering_avg:.4f}\",\n",
        "    str(triangles_count),\n",
        "    f\"{closed_fraction:.5f}\",\n",
        "    str(graph_diameter),\n",
        "    f\"{eff_diameter:.1f}\"\n",
        "]\n",
        "\n",
        "gt_display = [\n",
        "    str(ground_truth['nodes']),\n",
        "    str(ground_truth['edges']),\n",
        "    f\"{ground_truth['wcc_size']} ({ground_truth['wcc_size']/ground_truth['nodes']:.3f})\",\n",
        "    f\"{ground_truth['wcc_edges']} ({ground_truth['wcc_edges']/ground_truth['edges']:.3f})\",\n",
        "    f\"{ground_truth['scc_size']} ({ground_truth['scc_size']/ground_truth['nodes']:.3f})\",\n",
        "    f\"{ground_truth['scc_edges']} ({ground_truth['scc_edges']/ground_truth['edges']:.3f})\",\n",
        "    str(ground_truth['clustering']),\n",
        "    str(ground_truth['triangles']),\n",
        "    str(ground_truth['closed_fraction']),\n",
        "    str(ground_truth['diameter']),\n",
        "    str(ground_truth['eff_diameter'])\n",
        "]\n",
        "\n",
        "# Create results DataFrame\n",
        "results_data = {\n",
        "    'Metric': [\n",
        "        'Nodes',\n",
        "        'Edges',\n",
        "        'Largest WCC (nodes)',\n",
        "        'Largest WCC (edges)',\n",
        "        'Largest SCC (nodes)',\n",
        "        'Largest SCC (edges)',\n",
        "        'Avg. clustering coefficient',\n",
        "        'Number of triangles',\n",
        "        'Fraction of closed triangles',\n",
        "        'Diameter',\n",
        "        'Effective diameter (90-percentile)'\n",
        "    ],\n",
        "    'Ground Truth': gt_display,\n",
        "    'Computed': computed_display,\n",
        "    'Difference': differences\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results_data)\n",
        "print(\"\\n\" + results_df.to_string(index=False))\n",
        "\n",
        "# Calculate accuracy summary using Python built-ins\n",
        "print(f\"\\nSUMMARY:\")\n",
        "print(f\"Number of metrics: {len(differences)}\")\n",
        "\n",
        "# Convert to lists and use builtins module\n",
        "diff_list = list(differences)  # Ensure it's a regular Python list\n",
        "perfect_count = builtins.sum(1 for d in diff_list if d == 0)\n",
        "close_count = builtins.sum(1 for d in diff_list if d < 0.001)\n",
        "max_diff = builtins.max(diff_list)\n",
        "\n",
        "print(f\"Perfect matches (difference = 0): {perfect_count}\")\n",
        "print(f\"Close matches (difference < 0.001): {close_count}\")\n",
        "print(f\"Maximum difference: {max_diff}\")\n",
        "\n",
        "# Show which metrics have perfect matches\n",
        "perfect_matches = [results_data['Metric'][i] for i, d in enumerate(diff_list) if d == 0]\n",
        "if perfect_matches:\n",
        "    print(f\"Perfect matches: {', '.join(perfect_matches)}\")\n",
        "\n",
        "# Show metrics with largest differences\n",
        "if len(diff_list) > 0:\n",
        "    sorted_indices = sorted(range(len(diff_list)), key=lambda i: diff_list[i], reverse=True)\n",
        "    print(f\"\\nLargest differences:\")\n",
        "    for i in sorted_indices[:3]:  # Top 3 largest differences\n",
        "        print(f\"  {results_data['Metric'][i]}: {diff_list[i]:.6f}\")\n",
        "\n",
        "# ============================================\n",
        "# DETAILED COMPARISON REPORT\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"DETAILED COMPARISON REPORT - MANUAL IMPLEMENTATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define accuracy categories\n",
        "def get_accuracy_category(diff, metric_type='general'):\n",
        "    \"\"\"Categorize accuracy based on difference magnitude\"\"\"\n",
        "    if diff == 0:\n",
        "        return \"Perfect\"\n",
        "    elif diff < 0.001:\n",
        "        return \"Excellent\"\n",
        "    elif diff < 0.01:\n",
        "        return \"Very Good\"\n",
        "    elif diff < 0.1:\n",
        "        return \"Good\"\n",
        "    elif diff < 1.0:\n",
        "        return \"Fair\"\n",
        "    else:\n",
        "        return \"Poor\"\n",
        "\n",
        "# Detailed analysis for each metric\n",
        "metrics_analysis = [\n",
        "    {\n",
        "        'name': 'Nodes',\n",
        "        'ground_truth': ground_truth['nodes'],\n",
        "        'computed': node_count,\n",
        "        'difference': diff_list[0],\n",
        "        'accuracy': get_accuracy_category(diff_list[0]),\n",
        "        'discussion': \"Perfect match. Node counting is deterministic using simple union of source and destination nodes.\"\n",
        "    },\n",
        "    {\n",
        "        'name': 'Edges',\n",
        "        'ground_truth': ground_truth['edges'],\n",
        "        'computed': edge_count,\n",
        "        'difference': diff_list[1],\n",
        "        'accuracy': get_accuracy_category(diff_list[1]),\n",
        "        'discussion': \"Perfect match. Edge counting using count() on deduplicated edges DataFrame is exact.\"\n",
        "    },\n",
        "    {\n",
        "        'name': 'Largest WCC (nodes)',\n",
        "        'ground_truth': ground_truth['wcc_size'],\n",
        "        'computed': wcc_size,\n",
        "        'difference': diff_list[2],\n",
        "        'accuracy': get_accuracy_category(diff_list[2]),\n",
        "        'discussion': \"Result from manual BFS-based WCC implementation. Accuracy depends on: (1) Complete BFS convergence across all iterations, (2) Proper handling of undirected edges (both directions), (3) Correct component label propagation. Any discrepancy suggests incomplete graph traversal or early termination.\"\n",
        "    },\n",
        "    {\n",
        "        'name': 'Largest WCC (edges)',\n",
        "        'ground_truth': ground_truth['wcc_edges'],\n",
        "        'computed': wcc_edges,\n",
        "        'difference': diff_list[3],\n",
        "        'accuracy': get_accuracy_category(diff_list[3]),\n",
        "        'discussion': \"Edge counting within identified WCC. Accuracy depends on: (1) Correct WCC node identification (from previous step), (2) Proper edge filtering where both endpoints are in the component. Discrepancy would indicate issues in component membership or edge filtering logic.\"\n",
        "    },\n",
        "    {\n",
        "        'name': 'Largest SCC (nodes)',\n",
        "        'ground_truth': ground_truth['scc_size'],\n",
        "        'computed': scc_size,\n",
        "        'difference': diff_list[4],\n",
        "        'accuracy': get_accuracy_category(diff_list[4]),\n",
        "        'discussion': \"Result from manual Kosaraju's algorithm implementation. Accuracy depends on: (1) Correct first DFS traversal and finish time computation, (2) Proper graph transposition, (3) Correct second DFS on transposed graph in reverse finish time order, (4) Complete convergence of both DFS phases. Large discrepancy suggests algorithmic implementation errors.\"\n",
        "    },\n",
        "    {\n",
        "        'name': 'Largest SCC (edges)',\n",
        "        'ground_truth': ground_truth['scc_edges'],\n",
        "        'computed': scc_edges,\n",
        "        'difference': diff_list[5],\n",
        "        'accuracy': get_accuracy_category(diff_list[5]),\n",
        "        'discussion': \"Edge counting within identified SCC. Depends on: (1) Correct SCC node identification from Kosaraju's algorithm, (2) Proper directed edge filtering (both src and dst must be in SCC). Note: Only directed edges within the component are counted.\"\n",
        "    },\n",
        "    {\n",
        "        'name': 'Avg. clustering coefficient',\n",
        "        'ground_truth': ground_truth['clustering'],\n",
        "        'computed': clustering_avg,\n",
        "        'difference': diff_list[6],\n",
        "        'accuracy': get_accuracy_category(diff_list[6]),\n",
        "        'discussion': f\"Manual implementation using neighbor intersection method. Potential sources of discrepancy: (1) Handling of nodes with degree < 2 (should be excluded or set to 0), (2) Triangle counting methodology - using set intersection vs edge lookups, (3) Floating-point precision in division operations, (4) Possible double-counting of edges in undirected graph interpretation, (5) Aggregation and averaging methodology in distributed Spark environment.\"\n",
        "    },\n",
        "    {\n",
        "        'name': 'Number of triangles',\n",
        "        'ground_truth': ground_truth['triangles'],\n",
        "        'computed': triangles_count,\n",
        "        'difference': diff_list[7],\n",
        "        'accuracy': get_accuracy_category(diff_list[7]),\n",
        "        'discussion': \"Manual triangle counting implementation. Potential issues: (1) Triple-counting prevention (each triangle counted once, not thrice), (2) Proper handling of directed vs undirected edges, (3) Efficient set intersection logic for finding closed triangles, (4) Correct deduplication of triangle instances, (5) Complete graph coverage without missing any triangles. Large discrepancy suggests fundamental algorithmic issues.\"\n",
        "    },\n",
        "    {\n",
        "        'name': 'Fraction of closed triangles',\n",
        "        'ground_truth': ground_truth['closed_fraction'],\n",
        "        'computed': closed_fraction,\n",
        "        'difference': diff_list[8],\n",
        "        'accuracy': get_accuracy_category(diff_list[8]),\n",
        "        'discussion': f\"Significant discrepancy likely due to formula interpretation. Should be: (3 × triangles) / (3 × triangles + open_triplets) or equivalently triangles / (triangles + open_triplets/3). Common errors: (1) Using wrong denominator (total_triplets vs sum of closed and open), (2) Not accounting for the factor of 3 (each triangle appears in 3 triplets), (3) Incorrect open triplet counting, (4) Confusion with clustering coefficient formula. Requires methodology review.\"\n",
        "    },\n",
        "    {\n",
        "        'name': 'Diameter',\n",
        "        'ground_truth': ground_truth['diameter'],\n",
        "        'computed': graph_diameter,\n",
        "        'difference': diff_list[9],\n",
        "        'accuracy': get_accuracy_category(diff_list[9]),\n",
        "        'discussion': f\"2-sweep BFS approximation on undirected graph. Potential issues: (1) Limited iteration count (may not reach all nodes in 12 iterations), (2) Working on SCC subset ({scc_size} nodes) instead of full WCC ({wcc_size} nodes) - diameter should be computed on largest WCC, not SCC, (3) Graph conversion to undirected may have issues, (4) BFS convergence detection may terminate early, (5) Distance calculation errors. Key issue: Computing on wrong component!\"\n",
        "    },\n",
        "    {\n",
        "        'name': 'Effective diameter (90-percentile)',\n",
        "        'ground_truth': ground_truth['eff_diameter'],\n",
        "        'computed': eff_diameter,\n",
        "        'difference': diff_list[10],\n",
        "        'accuracy': get_accuracy_category(diff_list[10]),\n",
        "        'discussion': f\"90th percentile of sampled shortest paths. Issues: (1) Using SCC instead of WCC for calculation, (2) Sample size (20%) may be too small or unrepresentative, (3) Two-sweep sampling may bias results, (4) Limited BFS iterations affect path length accuracy, (5) Percentile calculation on combined samples from two sweeps may not represent true distribution. Should sample more node pairs or use ANF (Approximate Neighborhood Function) for better accuracy.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Print detailed analysis\n",
        "print(\"\\nMETRIC-BY-METRIC ANALYSIS:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for i, analysis in enumerate(metrics_analysis):\n",
        "    print(f\"\\n{i+1}. {analysis['name']}\")\n",
        "    print(f\"   Ground Truth: {analysis['ground_truth']}\")\n",
        "    print(f\"   Computed:     {analysis['computed']}\")\n",
        "    print(f\"   Difference:   {analysis['difference']:.6f}\")\n",
        "    print(f\"   Accuracy:     {analysis['accuracy']}\")\n",
        "    print(f\"   Discussion:   {analysis['discussion']}\")\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"OVERALL ASSESSMENT - MANUAL IMPLEMENTATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "accuracy_counts = {}\n",
        "for analysis in metrics_analysis:\n",
        "    acc = analysis['accuracy']\n",
        "    accuracy_counts[acc] = accuracy_counts.get(acc, 0) + 1\n",
        "\n",
        "print(f\"Accuracy Distribution:\")\n",
        "for acc_level in ['Perfect', 'Excellent', 'Very Good', 'Good', 'Fair', 'Poor']:\n",
        "    if acc_level in accuracy_counts:\n",
        "        print(f\"  {acc_level:12}: {accuracy_counts[acc_level]:2d} metrics ({100*accuracy_counts[acc_level]/len(metrics_analysis):5.1f}%)\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nOverall Assessment: {perfect_count}/{len(diff_list)} metrics perfect, \"\n",
        "      f\"{builtins.sum(1 for d in diff_list if d < 0.01)}/{len(diff_list)} within 1% accuracy\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}